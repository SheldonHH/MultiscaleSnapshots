{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess the reddit dataset\n",
    "\n",
    "In the following, we will preprocess the Social Network: Reddit Hyperlink Network from [SNAP Stanford](https://snap.stanford.edu/data/soc-RedditHyperlinks.html). The first step preprocess the dataset to a list of `networkX` graphs.\n",
    "\n",
    "1. Please download the [soc-redditHyperlinks-body.tsv](https://snap.stanford.edu/data/soc-redditHyperlinks-body.tsv) from the repository and move the file to the same directoy as the `preprocess.ipynb` notebook. \n",
    "2. You have to also install the Python packages of the `requirements.txt` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'soc-redditHyperlinks-body.tsv'\n",
    "df = pd.read_csv(path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PROPERTIES', 'POST_ID'], axis = 1, inplace = True)\n",
    "df['TIMESTAMP']= pd.to_datetime(df['TIMESTAMP']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_subreddit = pd.unique(df[['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT']].values.ravel('K'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['leagueoflegends', 'theredlion', 'inlandempire', ..., 'dildohero',\n",
       "       'hatedaddyofive', 'ouija_irl'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dict = {}\n",
    "for i in np.arange(len(unique_subreddit)): \n",
    "    keys_dict[unique_subreddit[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SOURCE'] = df[['SOURCE_SUBREDDIT']].values\n",
    "df['TARGET'] = df[['TARGET_SUBREDDIT']].values\n",
    "df[['SOURCE']] = np.vectorize(keys_dict.get)(df[['SOURCE']].values)\n",
    "df[['TARGET']] = np.vectorize(keys_dict.get)(df[['TARGET']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the graphs into intervals of one hour\n",
    "df_group = df.groupby([df['TIMESTAMP'].dt.date, df['TIMESTAMP'].dt.hour])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, group in df_group:\n",
    "    G = nx.Graph(time=t)\n",
    "    for row_index, row in group.iterrows():\n",
    "        G.add_node(row['SOURCE'], name=row['SOURCE_SUBREDDIT'])\n",
    "        G.add_node(row['TARGET'], name=row['TARGET_SUBREDDIT'])\n",
    "        G.add_edge(row['SOURCE'], row['TARGET'], sentiment=row['LINK_SENTIMENT'], time=row['TIMESTAMP'])\n",
    "    graphs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28979"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of graphs\n",
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for performance reasons we are only taking the last 2000 graphs here \n",
    "graphs = graphs[-2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute the graph layout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 12986.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout done\n"
     ]
    }
   ],
   "source": [
    "# G = nx.compose_all(graphs)\n",
    "G = nx.Graph()\n",
    "for graph in tqdm(graphs):\n",
    "    G.add_nodes_from(graph.nodes(data=True))\n",
    "    G.add_edges_from(graph.edges(data=True))\n",
    "\n",
    "coordinates = nx.spring_layout(G, iterations=100)\n",
    "\n",
    "# -- Alternative graph layouts for example --\n",
    "# coordinates = nx.kamada_kawai_layout(G)\n",
    "# coordinates = nx.spectral_layout(G)\n",
    "# coordinates = nx.circular_layout(G)\n",
    "# coordinates = nx.random_layout(G)\n",
    "\n",
    "print('Layout done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:14<00:00, 136.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# coordinates to list     \n",
    "coordinates = {k: v.tolist() for k, v in coordinates.items()}\n",
    "# modify positions \n",
    "for graph in tqdm(graphs):\n",
    "    nx.set_node_attributes(graph, coordinates, 'coord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'reddit_graphs.pkl'\n",
    "pickle.dump(graphs, open( filename, 'wb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute the graph embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, the temporal summaries are computed and the graph embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import Graph2Vec, GL2Vec, FGSD\n",
    "import itertools\n",
    "import datetime \n",
    "from collections import Counter\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snapshot:\n",
    "    def __init__(self, graphs, indx1, indx2):\n",
    "        \"\"\"Initialize snapshot from a list of graphs.\n",
    "\n",
    "            Keyword arguments:\n",
    "            graphs -- list of networkX graphs \n",
    "            indx1 -- first index in the overall graph list\n",
    "            indx2 -- last index in the overall graph list \n",
    "        \"\"\"\n",
    "        self.graphs = graphs[indx1:indx2]\n",
    "        self.indx1 = indx1\n",
    "        self.indx2 = indx2\n",
    "        self.time1 = datetime.datetime.combine(self.graphs[0].graph['time'][0], datetime.time(self.graphs[0].graph['time'][1].item()))\n",
    "        self.time2 = datetime.datetime.combine(self.graphs[-1].graph['time'][0], datetime.time(self.graphs[-1].graph['time'][1].item()))\n",
    "        self.duration = self.time2 - self.time1\n",
    "        self.union_g = None\n",
    "        \n",
    "        # occurences of nodes over time in a dict \n",
    "        nodes = []\n",
    "        for g in self.graphs:\n",
    "            nodes.append(g.nodes)\n",
    "        # get number of occurences \n",
    "        self.node_occ = Counter(x for xs in nodes for x in set(xs))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Snapshot: ' + str(self.time1) + ' - ' + str(self.time2) \n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Snapshot: ' + str(self.time1) + ' - ' + str(self.time2) \n",
    "    \n",
    "    def union_graph(self):\n",
    "        \"\"\"Return union graph.\n",
    "        \"\"\"\n",
    "        if not self.union_g:\n",
    "            G = nx.Graph()\n",
    "            for graph in self.graphs:\n",
    "                G.add_nodes_from(graph.nodes(data=True))\n",
    "                G.add_edges_from(graph.edges(data=True))\n",
    "            self.union_g = G\n",
    "        return self.union_g\n",
    "    \n",
    "    def disjoint_graph(self, num):\n",
    "        \"\"\"Return disjoint graph.\n",
    "\n",
    "            Keyword arguments:\n",
    "            num -- number of occurences in the sequence of graphs required to be in the disjoint graph (below the number)\n",
    "        \"\"\"\n",
    "        # filter out all occurence values below 1 and return nodes\n",
    "        nodes_dict = {x : self.node_occ[x] for x in self.node_occ if self.node_occ[x] <= num}\n",
    "        # union graph \n",
    "        union_g = self.union_g\n",
    "        \n",
    "        # return the subgraph matching all the nodes dict \n",
    "        return union_g.subgraph([*nodes_dict])\n",
    "    \n",
    "    def intersection_graph(self, num):\n",
    "        \"\"\"Return intersection graph.\n",
    "\n",
    "            Keyword arguments:\n",
    "            num -- number of occurences in the sequence of graphs required to be in the intersection graph \n",
    "        \"\"\"\n",
    "        nodes_dict = {x : self.node_occ[x] for x in self.node_occ if self.node_occ[x] >= num}\n",
    "        # union graph \n",
    "        union_g = self.union_g\n",
    "        \n",
    "        # return the subgraph matching all the nodes dict \n",
    "        return union_g.subgraph([*nodes_dict])\n",
    "    \n",
    "    def get_summaries(self, num):\n",
    "        \"\"\"Return all graph summaries in a list.\n",
    "\n",
    "            Keyword arguments:\n",
    "            num -- number of occurences in, please see intersection, and disjoint methods \n",
    "        \"\"\"\n",
    "        return [self.union_graph(), self.disjoint_graph(num), self.intersection_graph(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Level:\n",
    "    def __init__(self, graphs, level):\n",
    "        \"\"\"Initialize a level from from a list of graphs.\n",
    "\n",
    "            Keyword arguments:\n",
    "            graphs -- list of networkX graphs \n",
    "            level -- number for the level used to create window size \n",
    "        \"\"\"\n",
    "        self.graphs = graphs\n",
    "        self.level = level \n",
    "        self.window_size = int(math.pow(2, (level-1)))\n",
    "        self.overlap = int(self.window_size/2) \n",
    "        \n",
    "        # initialize the snapshots  \n",
    "        if self.window_size < 1: \n",
    "            raise ValueError('Window size of level below 1')\n",
    "        if self.overlap > 0:\n",
    "            self.snapshots = []\n",
    "            # the following if statement should be only false for the highest level of the hierarchy \n",
    "            if len(self.graphs) > self.window_size: \n",
    "                for i in range(0, len(self.graphs), self.overlap):\n",
    "                    self.snapshots.append(Snapshot(self.graphs, i, i+self.window_size))\n",
    "            # root node of hierarchy \n",
    "            else: \n",
    "                self.snapshots.append(Snapshot(self.graphs, 0, self.window_size))\n",
    "        else: \n",
    "            self.snapshots = self.graphs \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Level: ' + str(self.level) + ' - '  + str(self.window_size) + ' - ' + str(self.overlap) \n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Level: ' + str(self.level) + ' - '  + str(self.window_size) + ' - ' + str(self.overlap)\n",
    "    \n",
    "    def get_graphs(self):\n",
    "        \"\"\"Return all graphs of the level. Returns a dict with key:[union_graph, intersection_graph, disjoint_graph].\n",
    "        The key is the index in the snapshots array.\n",
    "        \"\"\"\n",
    "        print('Level: ' + str(self.level))\n",
    "        # num = int(self.overlap/2)\n",
    "        num = 2 # the subreddit hast to at least twice in the dataset\n",
    "        result = {}\n",
    "        for index, snap in tqdm(enumerate(self.snapshots)):\n",
    "            result[index] = snap.get_summaries(num)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hierarchy:\n",
    "    def __init__(self, graphs):\n",
    "        \"\"\"Initialize the hierarchy from a list of graphs.\n",
    "\n",
    "            Keyword arguments:\n",
    "            graphs -- list of networkX graphs \n",
    "        \"\"\"\n",
    "        self.graphs = graphs\n",
    "        self.levels = {}\n",
    "        self.height = 1\n",
    "        \n",
    "        window = 2\n",
    "        while window < len(self.graphs):\n",
    "            self.height = self.height + 1\n",
    "            window = int(math.pow(2, (self.height-1)))\n",
    "            self.levels[self.height] = Level(self.graphs, self.height)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.levels)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.levels\n",
    "    \n",
    "    def get_graphs(self):\n",
    "        \"\"\"Return all graphs of the hierarchy. Returns a dict of all graphs and a tuple of keys - refering to the elements.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for key, value in self.levels.items():\n",
    "            result[key] = value.get_graphs()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Hierarchy(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: Level: 2 - 2 - 1, 3: Level: 3 - 4 - 2, 4: Level: 4 - 8 - 4, 5: Level: 5 - 16 - 8, 6: Level: 6 - 32 - 16, 7: Level: 7 - 64 - 32, 8: Level: 8 - 128 - 64, 9: Level: 9 - 256 - 128, 10: Level: 10 - 512 - 256, 11: Level: 11 - 1024 - 512, 12: Level: 12 - 2048 - 1024}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 2157.67it/s]\n",
      "231it [00:00, 2308.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 1509.72it/s]\n",
      "122it [00:00, 1205.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:00, 731.18it/s] \n",
      "64it [00:00, 637.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:00, 349.02it/s]\n",
      "35it [00:00, 344.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:00, 330.90it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:00, 80.89it/s]\n",
      "9it [00:00, 86.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 90.87it/s]\n",
      "5it [00:00, 45.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 19.00it/s]\n",
      "3it [00:00, 24.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 26.31it/s]\n",
      "2it [00:00, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 14.09it/s]\n",
      "1it [00:00,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_graphs = h.get_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper method\n",
    "def flatten_dict(dd, separator='_', prefix=''):\n",
    "    return { str(prefix) + str(separator) + str(k) if prefix else k : v\n",
    "             for kk, vv in dd.items()\n",
    "             for k, v in flatten_dict(vv, separator, kk).items()\n",
    "             } if isinstance(dd, dict) else { prefix : dd }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_summary_graphs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dict and get the graphs and keys \n",
    "all_graphs = flatten_dict(all_graphs)\n",
    "graph_to_embed = [item for sublist in list(all_graphs.values()) for item in sublist]\n",
    "graph_keys = list(all_graphs.keys())\n",
    "# repeat three times as we use three summaries union, disjoint, intersection \n",
    "graph_keys = list(itertools.chain.from_iterable(itertools.repeat(x, num_summary_graphs) for x in graph_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11997"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Graph2Vec(workers=16, epochs=epochs)\n",
    "model.fit(graph_to_embed)\n",
    "embeddings = model.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GL2Vec(workers=20, epochs=epochs)\n",
    "#model.fit(graph_to_embed)\n",
    "#embeddings = model.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = FGSD(hist_bins=128)\n",
    "#model.fit(graph_to_embed)\n",
    "#embeddings = model.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11997\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'embeddings' : embeddings,\n",
    "          'keys' : graph_keys\n",
    "         }\n",
    "# keys are build with \"level_internal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'reddit_embeddings.pkl'\n",
    "pickle.dump(result, open( filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flask \n",
    "\n",
    "The two exportet files `reddit_graphs.pkl` and the `reddit_embeddings.pkl` are then used as inputs to the Flask application. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
